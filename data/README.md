# Datasets

Summary of viable speech-video datasets. The focus is datasets where the spoken language does not consist of 
curated descriptions or narrations, but simply of spontaneous speech in a visual context.

## SayCam

Audio and video recorded by head-mounted cameras of children, aged 6â€“32 months,
who wore a head-mounted camera for approximately 2 hr per week, over the course of approximately two-and-a-half years.
Around 500 hours total. Access via Databrary.
- Paper: https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00039/97495/SAYCam-A-Large-Longitudinal-Audiovisual-Dataset
- Data: https://nyu.databrary.org/volume/564

## AudioSet

Generic audio event dataset, consisting of 10 second clips extracted from YouTube videos. The speech category accounts for a ~2K hours of video.
- Paper: https://research.google.com/pubs/pub45857.html
- Data: https://research.google.com/audioset/index.html


## Spoken Moments in Time

These are clips with narrations, only useful for evaluation. 500K different short videos depicting a broad range of different events. 
Described orally to ensure that they remain as natural and concise as possible.
- Paper: https://openaccess.thecvf.com/content/CVPR2021/papers/Monfort_Spoken_Moments_Learning_Joint_Audio-Visual_Representations_From_Video_Descriptions_CVPR_2021_paper.pdf
- Data: http://moments.csail.mit.edu/spoken.html (cached locally)

